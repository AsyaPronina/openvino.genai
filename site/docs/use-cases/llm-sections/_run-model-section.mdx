import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import LLMPythonExample from './_llm-python-example.mdx';
import LLMCPPExample from './_llm-cpp-example.mdx';

## Run Model Using OpenVINOâ„¢ GenAI

`LLMPipeline` is the main object used for decoding. You can construct it straight away from the folder with the converted model.
It will automatically load the main model, tokenizer, detokenizer and default generation configuration.

<Tabs groupId="language">
    <TabItem label="Python" value="python">
        <Tabs groupId="device">
            <TabItem label="CPU" value="cpu">
                <LLMPythonExample device="CPU" />
            </TabItem>
            <TabItem label="GPU" value="gpu">
                <LLMPythonExample device="GPU" />
            </TabItem>
        </Tabs>
        
    </TabItem>
    <TabItem label="C++" value="cpp">
        <Tabs groupId="device">
            <TabItem label="CPU" value="cpu">
                <LLMCPPExample device="CPU" />
            </TabItem>
            <TabItem label="GPU" value="gpu">
                <LLMCPPExample device="GPU" />
            </TabItem>
        </Tabs>
        {/* ```cpp
        #include "openvino/genai/llm_pipeline.hpp"
        #include <iostream>
        
        int main(int argc, char* argv[]) {
            ov::genai::LLMPipeline pipe(model_path, "GPU");
            std::cout << pipe.generate("What is Large Language Model?", ov::genai::max_new_tokens(200));
        }
        ``` */}
    </TabItem>
</Tabs>

:::note

Use CPU or GPU as devices without any other code change.

:::
